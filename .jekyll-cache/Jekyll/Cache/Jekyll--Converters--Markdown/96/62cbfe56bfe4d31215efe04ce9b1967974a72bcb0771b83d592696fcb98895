I"!
<h1 id="ensembling">Ensembling</h1>
<p><strong>boosting VS bagging</strong></p>
<ol>
  <li>bagging 为放回抽样，多数表决或者平均 (Random Forest)</li>
  <li>树的集成本质上为boosting, boosting的实质是加性模型 (GBDT)</li>
  <li>boosting 下一棵树依赖上一棵树的训练, 树树之间只能串行
    <h1 id="boosting">Boosting</h1>
    <h2 id="definition">Definition</h2>
    <p><strong>Families of algorithms which convert weak learners to strong learners</strong></p>
  </li>
</ol>

<h2 id="步骤">步骤：</h2>
<ol>
  <li>初始：第一个弱学习机，所有点都初始化权重相同，预测</li>
  <li>过程：增加上一个学习机被预测错的点的权重，减少预测对的点的权重，进行下一个弱学习机</li>
  <li>整合：根据所有弱学习机的精度来决定各个模型的权重，整合结果</li>
</ol>

<p>几种算法:</p>
<ol>
  <li>AdaBoost (Adaptive Boosting)</li>
  <li>GBDT (Gradient Boosting Decision Tree)</li>
  <li>之前使用的是普通决策树(Desicion Trees), GBDT第一次引入了回归树CART(Classfication And Regression Trees)
一阶泰勒展开</li>
  <li>XGBoost</li>
  <li>相比GBDT, 显式引入正则项,约束决策树的复杂性</li>
  <li>二阶泰勒展开</li>
  <li>支持列抽样, 防止过拟合(与RF一样)</li>
</ol>

<h2 id="xgboost-extreme-gradient-boosting">XGBoost (eXtreme Gradient Boosting)</h2>
<h3 id="参数分类">参数分类：</h3>
<ol>
  <li>General Parameters: Guide the overall functioning</li>
  <li>Booster Parameters: Guide the individual booster (tree/regression) at each step</li>
  <li>Learning Task Parameters: Guide the optimization performed
Questions:
    <h3 id="qa">Q&amp;A</h3>
  </li>
  <li>xgboost的并行度</li>
  <li>选择特征最佳分裂点时, 进行枚举的时候并行; 树树串行</li>
  <li>learning rate</li>
  <li>实际为衰减因子, 为了适当减少前面树的影响, 用于控制过拟合(GBDT引入, 和梯度下降算法中的完全不同)</li>
  <li>xgboost防止过拟合的方法</li>
  <li>目标函数L2 正则</li>
  <li>列抽样,行抽样</li>
  <li>shrinkage 学习率</li>
</ol>

<h1 id="ref">Ref</h1>
<ol>
  <li><a href="https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/">Quick Introduction to Boosting Algorithms in Machine Learning</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">Complete Guide to Parameters Tunning in XGBoost with codes in Python</a></li>
</ol>
:ET