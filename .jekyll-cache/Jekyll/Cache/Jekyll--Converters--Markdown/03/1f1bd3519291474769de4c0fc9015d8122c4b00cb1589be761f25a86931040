I"m<h1 id="toc">TOC</h1>
<ol id="markdown-toc">
  <li><a href="#toc" id="markdown-toc-toc">TOC</a></li>
  <li><a href="#intuition-definition-and-notation" id="markdown-toc-intuition-definition-and-notation">Intuition, Definition and Notation</a></li>
  <li><a href="#algorithms" id="markdown-toc-algorithms">Algorithms</a>    <ol>
      <li><a href="#behavior-cloning" id="markdown-toc-behavior-cloning">Behavior Cloning</a></li>
      <li><a href="#direct-policy-learning-by-interactive-demonstrator" id="markdown-toc-direct-policy-learning-by-interactive-demonstrator">Direct Policy Learning (by interactive demonstrator)</a></li>
      <li><a href="#inverse-reinforcement-learning" id="markdown-toc-inverse-reinforcement-learning">Inverse Reinforcement Learning</a></li>
    </ol>
  </li>
  <li><a href="#applications" id="markdown-toc-applications">Applications</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ol>

<h1 id="intuition-definition-and-notation">Intuition, Definition and Notation</h1>

<p>Instead of learning from sparse rewards or specifying a manually reward function, the agent learns the optimal policy by learning, imitating the demonstrations of the experts.</p>

<p>Below shows the process:</p>

<p><img src="/img/DRL/imitation_learning_notation.png" width="600" /></p>

<p>Notations:</p>
<ol>
  <li>$s_t$ state of the environment at time t</li>
  <li>$o_t$ observation of the agent at time t</li>
  <li>$a_t$ action of agent at time t</li>
  <li>${\pi}_{\theta}(a_t | o_t)$: policy</li>
  <li>${\pi}_{\theta}(a_t | s_t)$: policy(if fully observed)</li>
  <li>$p(s_{t+1}|a_t, s_t)$: transiton fuctions to $s_{t+1}$ given $a_t$ and $s_t$</li>
</ol>

<h1 id="algorithms">Algorithms</h1>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Type</th>
      <th style="text-align: center">Advantage</th>
      <th style="text-align: center">Disadvantage</th>
      <th style="text-align: center">Use When</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Behavior Cloning</td>
      <td style="text-align: center">simple</td>
      <td style="text-align: center">no long-term planing  error can add up</td>
      <td style="text-align: center">the application is simple</td>
    </tr>
    <tr>
      <td style="text-align: center">Direct Policy Learning</td>
      <td style="text-align: center">long-term planing</td>
      <td style="text-align: center">interavtive expert demonstrations needed</td>
      <td style="text-align: center">application is complex; interactive demonstration is available</td>
    </tr>
    <tr>
      <td style="text-align: center">Inverse Reinforcement Learning</td>
      <td style="text-align: center">long-term planing no need for interavtive demenstration</td>
      <td style="text-align: center">can be hard to train</td>
      <td style="text-align: center">interavtive demonstartion is no available; easier to learn reward function then expert policy</td>
    </tr>
  </tbody>
</table>

<h2 id="behavior-cloning">Behavior Cloning</h2>

<p>Definition: learning the expert’s policy with supervised learning.</p>

<h2 id="direct-policy-learning-by-interactive-demonstrator">Direct Policy Learning (by interactive demonstrator)</h2>

<p>Definition: behavior cloning is a special of direct policy learning. First, supervised learning, then we roll out the game with the expert, and collect data. After that, we get more data demonstrated by the experts. Then go through the loop again.</p>

<p><img src="/img/DRL/IL.png" width="400" /></p>

<p>In the process of learning, the agent should ‘remembers’ all the mistakes that made. Two approches are used:</p>
<ol>
  <li>Data aggregation: agents are trained with all the history training data</li>
  <li>Policy aggregation: agents are only trained with data from the last iteration and then combined with policy from previous rounds wiht geometric blending.</li>
</ol>

<h2 id="inverse-reinforcement-learning">Inverse Reinforcement Learning</h2>

<p>Definition: learn the reward function from the expert demonstrations, the train the agent with RL algorithms.</p>

<p>Procedures are as follows:</p>

<p><img src="/img/DRL/process_of_IRL" width="400" /></p>

<p>Two type of IRL: model-free and model-given
<img src="/img/DRL/type_of_IRL" width="400" /></p>

<h1 id="applications">Applications</h1>

<ol>
  <li>Game AI</li>
  <li></li>
</ol>

<h1 id="reference">Reference</h1>
:ET