---
layout: post
title: Xgboost Explain
<!-- subtitle:  Reinforcement Learning Basic - Concepts and Taxonomy -->
date: 2020-05-25
published: True
mathjax: True
catalog: true
tags:
  - machine learning
---
# TOC
1. Introduction
{:toc}

# Introduction
In this post, I will try to summarize the details of xgboost algorithm. General principles of machine learning: define a objective function and optimize it.

# Objective Function

Normally the objective function consists of two parts: 

1. training loss
2. regularization, which controlled the complexity of the model

The formular is ${obj}(\theta) = L(\theta) + \Omega(\theta)\]=$, in which $\theta$ represents the paramters of the model.

For xgboost, the **objective function** is defined as:

$obj(\theta)=\sum_{i=1}^{n}l(y_i, \hat{y_i}) + \sum_{i=1}^{K}\Omega(f_k)$

where 
- n is the number of training samples
- K is the number of tree 
- $f_k$ is a CART tree function.



# To Get a New Tree

For the $t$th tree, the predicted value for $x_i$ is:

$y_i^{t} =\sum_{k=1}^{t}f_k(x_i)=y_i^{t-1}+f_t(x_i)$

The objective function $obj(\theta)$ at this point can be reduced to: $obj^{t}(\theta)=\sum_{i=1}^{n}l(y_i^{t}, y_i^{t-1}+f_t(x_i)) + \Omega(f_t) + constant$

By applying Taylor expansion and abandoning the constant term, **the specific objective at step t becomes:** 

$obj^{t}(\theta)=\sum_{i=1}^{n}l(g_if_i(x_i)+\frac{1}{2}h_if_i^{1}(x_i)) + \Omega(f_t)$

## Model Complexity
First let's refine the definition of a tree function at step t:

$f_t(x)=w_{q(x)}, w\in R^{T}, q(x): R^d \rightarrow {1, 2 \cdot, \cdot T}$

where 
- T: the number of leaves
- w: the vector of score in leaves
- q(x): a function that assign data point to the corresponding leaf

The complexity of the model is defined as:

$\Omega(f)= \gamma T+\frac{1}{2}\lambda\sum_{j=1}^{T}w_y^2$


## Target Objective
**The best objective reduction we can get is:**

$w_j^\ast = -\frac{G_j}{H_j+\lambda}$


$obj^\ast = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T$

When growing the $t$th tree, xgboost will greedly find the optimal split points that brings new gain to the objective function. The gain is defined as:

$Gain = Obj_{L+R}-(Obj_L + Obj_R)$

If Gain>0, then objective function decreases ???. 


# Optimal Split points

Steps to get the optimal split points:
1. Go though all the features
2. Sort it  find the optimal split points for every feature
2. 

In order to find the optimal split points when spl


# References




# Reference
1. [Xgboost, GBDT超详细推导](https://zhuanlan.zhihu.com/p/92837676)