---
layout: post
title: XGBoost Explain
<!-- subtitle:  Reinforcement Learning Basic - Concepts and Taxonomy -->
date: 2020-05-25
published: True
mathjax: True
catalog: true
tags:
  - machine learning
---
# TOC
1. Introduction
{:toc}

# Introduction
XGBoost is classified as a type of ensemble learning, which combines several weak learner into a strong learner. In the case of XGBoost, the weak learner is the Classification And Regression Tree (CART). 

The principle of Xgboost is the same as other supervised learning: 
1. first define a objective function
2. then optimize it. 

In the process of optimization, new tree will be created sequentially to reduce the objective function. The optimization will end when the objective function can't be reduced. Finally, the predicted value of an input is the summation of all the trees' output.

# Objective Function

For supervised learning, normally the objective function consists of two parts: 

1. training loss
2. regularization, which controlled the complexity of the model

The formular is ${obj}(\theta) = L(\theta) + \Omega(\theta)$, in which $\theta$ represents the paramters of the model.

## Objective Function for Xgboost

For xgboost, the **objective function** is defined as:

$obj(\theta)=\sum_{i=1}^{n}l(y_i, \hat{y_i}) + \sum_{i=1}^{K}\Omega(f_k)$

where 
- n is the number of training samples
- K is the number of tree 
- $f_k$ is a CART tree function.

## Objective Function When Growing a New Tree

### Train Loss Part

For the $t$th tree, the predicted value for $x_i$ is:

$y_i^{t} =\sum_{k=1}^{t}f_k(x_i)=y_i^{t-1}+f_t(x_i)$

The objective function $obj(\theta)$ at this point can be reduced to: $obj^{t}(\theta)=\sum_{i=1}^{n}l(y_i^{t}, y_i^{t-1}+f_t(x_i)) + \Omega(f_t) + constant$

By applying Taylor expansion and abandoning the constant term, **the specific objective at step t becomes:** 

$obj^{t}(\theta)=\sum_{i=1}^{n}l(g_if_i(x_i)+\frac{1}{2}h_if_i^{1}(x_i)) + \Omega(f_t)$


### Regularization Part

First let's refine the definition of a tree function at step t:

$f_t(x)=w_{q(x)}, w\in R^{T}, q(x): R^d \rightarrow {1, 2 \cdot, \cdot T}$

where 
- T: the number of leaves
- w: the vector of score in leaves
- q(x): a function that assign data point to the corresponding leaf

The complexity of the model is defined as:

$\Omega(f)= \gamma T+\frac{1}{2}\lambda\sum_{j=1}^{T}w_y^2$


### Target Objective Functions

**The best objective reduction we can get is:**

$w_j^\ast = -\frac{G_j}{H_j+\lambda}$


$obj^\ast = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T$

$w_j$ represents the score (weight) of a leaf.



# Optimazation

When growing the $t$th tree, it's not practice to enumerate all possible tree and pick the best one. Instead, xgboost will greedly find the optimal split points that brings new gain to the objective function. The gain is defined as:

$Gain = Obj_{L+R}-(Obj_L + Obj_R)$

If Gain>0, then objective function increases

## Optimal Split points

Steps to get the optimal split points:
1. Enumerate all the features
2. Sort the value of every feature 
3. Find the optimal split points for every feature
2. Find the optimal spllit feature and feature value

Some approches are used to improve efficiency:
1. 特征预排序+缓存：XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构，后面的迭代中会重复地使用这个结构，使计算量大大减小。
2. 分位点近似法：对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。
3. 并行查找：由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。

## End Optimization 
When the criterions blew are meet, the growth of tree will be ended:
1. The gain a new split brings is small than zero
2. The depth of tree reach $max\_depth$
3. When the score (weight) is smaller than $min\_child\_weight$

# Reference
1. [Xgboost, GBDT超详细推导](https://zhuanlan.zhihu.com/p/92837676)
2. [Xgboost Documentation - Introduction to Boosted Tree](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)
3. [XGBoos - A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)